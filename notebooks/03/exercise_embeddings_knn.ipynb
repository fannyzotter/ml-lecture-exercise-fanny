{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"../../assets/htwlogo.svg\">\n",
    "\n",
    "# Exercise: Using embeddings and implementing a k-nearest neighbour classifier\n",
    "\n",
    "We already learned a lot about embeddings and also got to know our first classication\n",
    "model - a k-nearest neighbour classifier. Let's dive into some practice and play around with CLIP embeddings.\n",
    "\n",
    "**Author**: _Erik Rodner_<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's time to use pytorch, at least to handle CLIP embeddings,\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel # HuggingFace module\n",
    "# and some interesting datasets.\n",
    "from torchvision import datasets\n",
    "# For classical splitting, we will still use sklearn, since we\n",
    "# learned about it. Otherwise, one would rather use pytorch only later on.\n",
    "from sklearn.model_selection import train_test_split\n",
    "# This is just to compute pairwise distances for the first task\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not have to start from scratch at all. The following functions should be known from the lecture\n",
    "material. However, there is a slight change in the ``get_text_embeddings``function, which allows processing\n",
    "several strings at once. The ``get_pairwise_distances`` function is just for your convenience, it basically computes\n",
    "all mutual distances between a set of points and returns the distances as a quadratic matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CLIP model and processor - this can take a while, since the model weights need\n",
    "# to be downloaded\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Function to get the text embedding using CLIP\n",
    "def get_text_embeddings(text_array):\n",
    "    \"\"\" Get CLIP-embeddings for a list of strings, returns a list of numpy arrays \"\"\"\n",
    "    \n",
    "    # Preprocess the text\n",
    "    inputs = processor(text=text_array, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # Generate the embedding\n",
    "    with torch.no_grad():\n",
    "        outputs = model.get_text_features(**inputs)\n",
    "    # Return the embeddings as an array of numpy arrays\n",
    "    return [ output.cpu().numpy().flatten() for output in outputs ]\n",
    "\n",
    "def get_image_embedding(image):\n",
    "    \"\"\" Get a CLIP-embedding for a single PIL image \"\"\"\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    # Generate the embedding\n",
    "    with torch.no_grad():\n",
    "        outputs = model.get_image_features(**inputs)\n",
    "    # Return the embedding as a numpy array\n",
    "    return outputs[0].cpu().numpy().flatten()\n",
    "\n",
    "def get_pairwise_distances(vectors):\n",
    "    \"\"\" Compute mutual distances and return them as a quadratic matrix \"\"\"\n",
    "    return squareform(pdist(vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Analyze CLIP embeddings for a collection of texts\n",
    "\n",
    "The first task is about exploration. take a list of texts as follows and try to understand the resulting\n",
    "pairwise distances. The distances should somehow match with your understanding of semantic similarity. What are the dimensions of these vectors?\n",
    "Can you find counter-examples? How about visualizing them in 2D using PCA or ``umap``(not officially part of the lecture)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vectors = get_text_embeddings([\"Hello\", \"How are you?\", \"cat\", \"dog\", \"butterfly\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pairwise_distances(embedding_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation for the next tasks\n",
    "\n",
    "In the following, we will explore CLIP embeddings of a classical computer vision datasets that\n",
    "lived even before ImageNet: Caltech 101. It has 101 object categories annotated for classification and most\n",
    "of the images were collected using Google image search. Pytorch has some nice helper functions to automatically download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.Caltech101(root=\"../data\", download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task will be about **zero-shot performance** of CLIP embeddings. This is only possible, because CLIP provides a joint embedding space of images and text and semantic similarity was enforced of images and their captions during training.\n",
    "Therefore, we can easily compute the semantic similarity of an image and a text, by calculating the distance\n",
    "between their embeddings. \n",
    "\n",
    "We will apply this principle to the dataset, by computing the similarity of each image with all possible category names. The category with the smallest distance (or highest similarity) to the image will be our prediction.\n",
    "\n",
    "So let's first compute all embeddings of all category names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = get_text_embeddings(dataset.categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our distance metric, we could use standard Euclidean distance. However, it is reasonable to use the\n",
    "so called cosine similarity function, defined as follows (can you spot why it is called cosine similarity?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(x1, x2):\n",
    "    return np.dot(x1, x2)/(np.linalg.norm(x1) * np.linalg.norm(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's all select a few training and testing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = range(len(dataset))\n",
    "labels = [ example[1] for example in dataset ]\n",
    "indices_train, indices_temp, _, labels_temp  = train_test_split(indices, labels, shuffle=True, stratify=labels, train_size=101*4, random_state=42)\n",
    "_, indices_test = train_test_split(indices_temp, test_size=202, stratify=labels_temp, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Zero-shot classification on Caltech 101\n",
    "\n",
    "In the following, we only use the test dataset. Do the following:\n",
    "1. Go through all test images (I helped with that already :P)\n",
    "2. Compute the image embedding of each test image\n",
    "3. Calculate the cosine similarity of the image embedding and each category embedding\n",
    "4. Obtain a class prediction by looking at the maximum similarity\n",
    "5. Compare it with the ground-truth label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_test in indices_test:\n",
    "    image, label = dataset[index_test]\n",
    "    # your code comes here :)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Write your own k-nn classifier\n",
    "\n",
    "Let's skip all text embeddings in the following, we will only use image embeddings.\n",
    "Your task is now to implement a k-nearest neighbour classifier and evaluate this later on with the above specified training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code comes here :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Can you spot the dataset bias?\n",
    "\n",
    "Caltech 101 has some severe dataset biases - can you spot them?\n",
    "Look at the images of the motorbike or airplane category!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some visualization code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-exercise-pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
