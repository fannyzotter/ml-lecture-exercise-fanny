{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"../../assets/htwlogo.svg\">\n",
    "\n",
    "# Exercise: Derivatives and linear models\n",
    "\n",
    "In the following Jupyter notebook, we test our knowledge of derivatives and linear models. Ready?\n",
    "\n",
    "**Author**: _Erik Rodner_<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Derive and compute derivatives of a multivariate function\n",
    "\n",
    "Our goal in this exercise is to compute the partial derivatives of a function $f(x,y)$.\n",
    "Implement the following functions and their respective gradients (code template follows in the next cell):\n",
    "1. $f(x,y) = 3x^2 + 2xy + y^2$\n",
    "2. $f(x,y) = (x-3e^{-xy})^2$\n",
    "3. $f(x,y) = log(x^2 - 4x + 4)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_to_diff(x, y):\n",
    "    return 3*x**2 + 2*x*y + y**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is now to compute the partial derivatives of the function, the following method needs to return\n",
    "the gradient vector at location (x,y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y):\n",
    "    df_dx = 0 # modify this code!\n",
    "    df_dy = 0 # modify this code!\n",
    "    return np.array([df_dx, df_dy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we need to check the correctness of the gradients. One way to test the gradients is too check whether they are close\n",
    "to finite difference:\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} \\approx \\frac{f(x + \\epsilon,y) - f(x-\\epsilon,y)}{2\\epsilon}\n",
    "$$\n",
    "\n",
    "Remark: there are different ways for approximating the gradients with finite difference, such as simple forward differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute numerical gradient using finite differences\n",
    "def compute_numerical_gradient(f, x, y, epsilon=1e-5):\n",
    "    \"\"\"Approximates the gradient (partial derivatives) using finite differences.\"\"\"\n",
    "    df_dx = (f(x + epsilon, y) - f(x - epsilon, y)) / (2 * epsilon)\n",
    "    df_dy = (f(x, y + epsilon) - f(x, y - epsilon)) / (2 * epsilon)\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "# Function to test the correctness using finite differences\n",
    "def test_gradient_func(x_test = 1.0, y_test = 2.0):\n",
    "    expected_gradient = compute_gradient(x_test, y_test)\n",
    "    numerical_gradient = compute_numerical_gradient(function_to_diff, x_test, y_test)\n",
    "    \n",
    "    if np.allclose(expected_gradient, numerical_gradient, atol=1e-6):\n",
    "        print(\"Test passed! The analytical gradient matches the numerical approximation.\")\n",
    "    else:\n",
    "        print(\"Test failed. Analytical gradient:\", expected_gradient, \n",
    "              \"Numerical gradient:\", numerical_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready to test your code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gradient_func()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Linear regression\n",
    "\n",
    "Implement linear regression using the normal equation method. **Fill in missing code to solve for weights using matrix operations.**\n",
    "\n",
    "In the following code, we use a simple but effective trick. Instead of taking care of the bias term seperately, we simply\n",
    "append a $1$ to the inputs $\\tilde{\\mathbf{x}} = [\\mathbf{x}, 1]^T = [x_1, x_2, \\ldots, x_D, 1]^T$. Afterwards, we simply use a linear model\n",
    "without bias term on $\\mathbf{x}'$: \n",
    "$$\\tilde{\\mathbf{w}}^T \\tilde{\\mathbf{x}} = [w_1, \\ldots, w_D, b]^T [x_1, x_2, \\ldots, x_D, 1]^T = \\mathbf{w}^T \\mathbf{x} + b$$ \n",
    "\n",
    "Summary: add a 1 to your inputs and skip thinking about the bias term (in most cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for regression\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10)\n",
    "\n",
    "# Add a bias term (intercept) to X\n",
    "X_b = np.c_[X, np.ones((X.shape[0], 1))]  # add x0 = 1 to each instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn, compute the best parameter vector $\\mathbf{w}$. It does not have to be a single line of code (but it is possible):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_best = np.zeros(X_b.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check your solution visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the computed weights\n",
    "print(\"Optimal weights (including bias):\", w_best)\n",
    "\n",
    "# Predict values using the derived model\n",
    "y_pred = X_b.dot(w_best)\n",
    "\n",
    "# Plotting the results\n",
    "plt.scatter(X, y, color='blue', label='Data points')\n",
    "plt.plot(X, y_pred, color='red', label='Linear regression line (Normal Equation)')\n",
    "plt.title('Linear Regression Optimization using Normal Equation')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Explore the stability of linear regression\n",
    "\n",
    "What happens if you add more noise to the features? What happens if you add outliers? Can you think about an algorithm that deals with these outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-exercise-pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
