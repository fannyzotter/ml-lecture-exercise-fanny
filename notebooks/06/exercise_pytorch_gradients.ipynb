{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"../../assets/htwlogo.svg\">\n",
    "\n",
    "# Exercise: using pytorch for non-linear optimization\n",
    "\n",
    "**Author**: Dive into Deep Learning, adapted by _Erik Rodner_<br>\n",
    "\n",
    "In the following exercise, we will dive into automatic differentiation and write\n",
    "a simple gradient descent optimizer on simple functions. In particular, we try to solve\n",
    "the linear equation system $\\mathbf{A} w = b$ with $\\mathbf{A}$ being a matrix and $w$ and $b$\n",
    "corresponding vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append(os.path.join(\"..\", \"..\", \"utils\"))\n",
    "\n",
    "from torchutils import make_dot\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pylab as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Definition of the linear equation system\n",
    "\n",
    "Let's define a matrix $\\mathbf{A}$ and a vector $b$ for our linear equation system that we would like to solve.\n",
    "\n",
    "Very advanced question: The matrix needs to have special properties, in order to make the iterative algorithm work - can you find out which ones? This is an advanced question out of scope for the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([.1, .6, .9])\n",
    "A = np.array([[ 3.98481808,  1.48175294, -0.53227922],\n",
    "       [ 1.48175294,  1.98511523,  0.84631116],\n",
    "       [-0.53227922,  0.84631116,  1.83430071]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Solving it the traditional way\n",
    "\n",
    "Do you remember linear algebra and how to solve a linear equation system? Traditionally you would use the Gauss algorithm, which is also available in numpy. Another more instable way is to use the inverse matrix: $\\mathbf{A}^{-1}b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.dot(A.T, b)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(np.dot(A, w)-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Solving the system with gradient descent\n",
    "\n",
    "The following method we will implement is to use gradient descent to minimize the squared error loss $\\epsilon(w) = \\|\\mathbf{A} w - b\\|^2$ with respect to $w$. For gradient descent, we need one essential thing: the gradient. However, in our case, we do not compute this by hand but rather use pytorchs autograd mechanism.\n",
    "\n",
    "First, let's define the required pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tA = torch.Tensor(np.array(A))\n",
    "tb = torch.Tensor(np.array(b))\n",
    "tw = torch.ones(3, requires_grad=True)\n",
    "tw.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Ready, steady, go!\n",
    "\n",
    "The following loop implements a gradient descent method on the corresponding quadratic loss for the linear equation system. Can you spot the errors here? You have to find a wrong statement and for performance reasons you need to add another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "losses = []\n",
    "for i in range(1000):\n",
    "    loss = torch.norm(torch.matmul(tA,tw))\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        print (f\"Loss in iteration {i}: {loss}\")\n",
    "        losses.append(loss.data)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        tw -= learning_rate * tw.grad    \n",
    "    tw.grad.zero_()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Visualize the solution and the optimization progress of the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"Solution from the non-linear optimization: {tw.data}\")\n",
    "print (f\"Solution from the algebraic method: {w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses[::10])\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Show the graph again of the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dot(loss, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
